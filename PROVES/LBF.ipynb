{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadb6122",
   "metadata": {},
   "source": [
    "# LBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca1919",
   "metadata": {},
   "source": [
    "python3 -m venv lbf-env\n",
    "source lbf-env/bin/activate\n",
    "pip install -e lb-foraging/\n",
    "\n",
    "python -m ipykernel install --user --name=lbf-env --display-name \"Python (lb-foraging)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba5e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lbforaging in c:\\users\\propietari\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.26.4)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.2.1)\n",
      "Requirement already satisfied: pyglet<2 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.5.31)\n",
      "Requirement already satisfied: six in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lbforaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845306bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(6), Discrete(6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0329921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Box([-1. -1.  0. -1. -1.  0. -1. -1.  0.], [7. 7. 4. 7. 7. 2. 7. 7. 2.], (9,), float32), Box([-1. -1.  0. -1. -1.  0. -1. -1.  0.], [7. 7. 4. 7. 7. 2. 7. 7. 2.], (9,), float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3601d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n"
     ]
    }
   ],
   "source": [
    "num_agents = env.unwrapped.n_agents\n",
    "print(f'Number of agents: {num_agents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e349f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Unable to save notebook session history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">frightful-broomstick-64</strong> at: <a href='https://wandb.ai/marionapla/LBF/runs/01lt8119' target=\"_blank\">https://wandb.ai/marionapla/LBF/runs/01lt8119</a><br> View project at: <a href='https://wandb.ai/marionapla/LBF' target=\"_blank\">https://wandb.ai/marionapla/LBF</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251031_155130-01lt8119\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\propietari\\Desktop\\TGF-MARL\\PROVES\\wandb\\run-20251031_155223-41ykcn1a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marionapla/LBF/runs/41ykcn1a' target=\"_blank\">dreadful-candy-65</a></strong> to <a href='https://wandb.ai/marionapla/LBF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marionapla/LBF' target=\"_blank\">https://wandb.ai/marionapla/LBF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marionapla/LBF/runs/41ykcn1a' target=\"_blank\">https://wandb.ai/marionapla/LBF/runs/41ykcn1a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Agent 0: obs_dim=18, act_dim=6\n",
      "Initialized Agent 1: obs_dim=18, act_dim=6\n",
      "Burn-in buffer with random actions...\n",
      "Actions taken: [4, 3]\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\propietari\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:245: UserWarning: WARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burn-in complete., 4]00\n",
      "Starting training...\n",
      "6\n",
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\propietari\\AppData\\Local\\Temp\\ipykernel_16172\\2178165419.py:123: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.BoolTensor(dones).to(self.qnet.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "tensor([104.9944,  98.1328,  98.2259, 102.1938, 102.8031, 102.6620],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "6\n",
      "6\n",
      "6\n",
      "tensor([103.0663,  97.9787, 100.5698, 105.9366, 100.8547, 106.3618],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "tensor([122.7260, 133.5234, 126.5396, 124.1794, 131.5818, 123.7723],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 223\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m agents, rewards_history\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# ---------------- Train ----------------\u001b[39;00m\n\u001b[1;32m--> 223\u001b[0m agents, rewards_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_iql\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_EPISODES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;66;03m# ---------------- Plot learning curves ----------------\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# plt.figure(figsize=(10, 5))\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# for i in range(env.unwrapped.n_agents):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# ---------------- Cleanup ----------------\u001b[39;00m\n\u001b[0;32m    236\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "Cell \u001b[1;32mIn[20], line 176\u001b[0m, in \u001b[0;36mtrain_iql\u001b[1;34m(env, n_episodes, device)\u001b[0m\n\u001b[0;32m    174\u001b[0m actions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(agents):\n\u001b[1;32m--> 176\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m     actions\u001b[38;5;241m.\u001b[39mappend(a)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Step environment\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[20], line 79\u001b[0m, in \u001b[0;36mDQN.get_action\u001b[1;34m(self, state, epsilon)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m):\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m epsilon:\n\u001b[1;32m---> 79\u001b[0m         \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mout_features)\n\u001b[0;32m     81\u001b[0m         \u001b[38;5;66;03m# return np.random.choice(self.actions)\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py:47\u001b[0m, in \u001b[0;36mStreamWrapper.write\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__convertor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py:177\u001b[0m, in \u001b[0;36mAnsiToWin32.write\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrip \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert:\n\u001b[1;32m--> 177\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_and_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    179\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39mwrite(text)\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py:205\u001b[0m, in \u001b[0;36mAnsiToWin32.write_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_ansi(\u001b[38;5;241m*\u001b[39mmatch\u001b[38;5;241m.\u001b[39mgroups())\n\u001b[0;32m    204\u001b[0m     cursor \u001b[38;5;241m=\u001b[39m end\n\u001b[1;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_plain_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\site-packages\\colorama\\ansitowin32.py:211\u001b[0m, in \u001b[0;36mAnsiToWin32.write_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m start \u001b[38;5;241m<\u001b[39m end:\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39mwrite(text[start:end])\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\site-packages\\ipykernel\\iostream.py:578\u001b[0m, in \u001b[0;36mOutStream.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(evt\u001b[38;5;241m.\u001b[39mset)\n\u001b[0;32m    577\u001b[0m     \u001b[38;5;66;03m# and give a timeout to avoid\u001b[39;00m\n\u001b[1;32m--> 578\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mevt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    579\u001b[0m         \u001b[38;5;66;03m# write directly to __stderr__ instead of warning because\u001b[39;00m\n\u001b[0;32m    580\u001b[0m         \u001b[38;5;66;03m# if this is happening sys.stderr may be the problem.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIOStream.flush timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, file\u001b[38;5;241m=\u001b[39msys\u001b[38;5;241m.\u001b[39m__stderr__)\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32mc:\\Users\\propietari\\anaconda3\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lbforaging \n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ---------------- WandB setup ----------------\n",
    "run = wandb.init(\n",
    "    project=\"LBF\",\n",
    "    config={\n",
    "        \"env_name\": \"Foraging-8x8-2p-4f-v3\",\n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# ---------------- Environment ----------------\n",
    "env_conf = \"Foraging-8x8-2p-4f-v3\"\n",
    "env = gym.make(env_conf)\n",
    "'''env = gym.make(\"Foraging<obs>-<x_size>x<y_size>-<n_agents>p-<food>f<force_c>-v1\")\n",
    "    • <obs>: This optional field can either be empty (\"\") or indicate a partially observable task\n",
    "    with visibility radius of two fields (\"-2s).\n",
    "    • <x_size>: This field indicates the horizontal size of the environment map and can by\n",
    "    default take any values between 5 and 20.\n",
    "    • <y_size>: This field indicates the vertical size of the environment map and can by default\n",
    "    take any values between 5 and 20. It should be noted, that upon import only environments\n",
    "    with square dimensions (<x_size> = <y_size>) are registered and ready for creation.\n",
    "    • <n_agents>: This field indicates the number of agents within the environment. By default,\n",
    "    any values between 2 and 5 are automatically registered.\n",
    "    • <food>: This field indicates the number of food items scattered within the environment. It\n",
    "    can take any values between 1 and 10 by default.\n",
    "    • <force_c>: This optional field can either be empty (\"\") or indicate a task with only\n",
    "    \"cooperative food\" (\"-coop\". In the latter case, the environment will only contain food of a\n",
    "    level such that all agents have to cooperate in order to pick the food up. This mode should\n",
    "    only be used with up to four agents.'''\n",
    "\n",
    "# ---------------- Hyperparameters ----------------\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 5000\n",
    "MAX_EPISODES = 20000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.995\n",
    "EPSILON_MIN = 0.05\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "BURN_IN = 1000\n",
    "DEVICE = 'cpu'  # 'cuda' if GPU available\n",
    "\n",
    "# ---------------- DQN Network ----------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, lr=LR, device=DEVICE):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        if device == 'cuda':\n",
    "            self.model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).to(self.device)\n",
    "        if x.ndim > 2:\n",
    "            x = x.view(x.size(0), -1)  # flatten multi-dim observations\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.model[-1].out_features)\n",
    "        else:\n",
    "            qvals = self.forward(state)\n",
    "            print(f'qvals: {qvals}  ')    #########\n",
    "            return torch.argmax(qvals).item()\n",
    "\n",
    "# ---------------- Replay Buffer ----------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition', ['state', 'action', 'reward', 'done', 'next_state'])\n",
    "    \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(self.transition(state, action, reward, done, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        return zip(*batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ---------------- Independent Q-Learning Agent ----------------\n",
    "class IQLAgent:\n",
    "    def __init__(self, obs_dim, act_dim, device=DEVICE, lr=LR, gamma=GAMMA, epsilon=EPSILON_START, eps_decay=EPSILON_DECAY):\n",
    "        self.qnet = DQN(obs_dim, act_dim, lr, device)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def store_transition(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "    \n",
    "    def update(self, batch_size=BATCH_SIZE):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.qnet.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(-1).to(self.qnet.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.qnet.device)\n",
    "        dones = torch.BoolTensor(dones).to(self.qnet.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.qnet.device)\n",
    "        \n",
    "        qvals = self.qnet(states).gather(1, actions)\n",
    "        q_next = self.qnet(next_states).max(dim=1)[0].detach()\n",
    "        q_next[dones] = 0\n",
    "        target = rewards + self.gamma * q_next\n",
    "        \n",
    "        loss = nn.MSELoss()(qvals.squeeze(), target)\n",
    "        self.qnet.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.qnet.optimizer.step()\n",
    "\n",
    "# ---------------- Multi-Agent Training Loop ----------------\n",
    "def train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE):\n",
    "    num_agents = env.unwrapped.n_agents\n",
    "    agents = []\n",
    "    for i in range(num_agents):\n",
    "        obs_dim = np.prod(env.observation_space[i].shape)\n",
    "        act_dim = env.action_space[i].n\n",
    "        agents.append(IQLAgent(obs_dim, act_dim, device=device))\n",
    "        print(f\"Initialized Agent {i}: obs_dim={obs_dim}, act_dim={act_dim}\")\n",
    "    \n",
    "    rewards_history = [[] for _ in range(num_agents)]\n",
    "\n",
    "    # ----------- Burn-in Phase -----------\n",
    "    print(\"Burn-in buffer with random actions...\")\n",
    "    steps = 0\n",
    "    while steps < BURN_IN:\n",
    "        print(f\"Burn-in steps: {steps}/{BURN_IN}\", end='\\r')\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [env.action_space[i].sample() for i in range(num_agents)]\n",
    "            print(f\"Actions taken: {actions}\", end='\\r')\n",
    "            next_obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            done = np.any(terminated) or np.any(truncated)\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i].flatten(), actions[i], rewards[i], done, next_obs[i].flatten())\n",
    "            obs = next_obs\n",
    "            steps += 1\n",
    "    print(\"Burn-in complete.\\nStarting training...\")\n",
    "\n",
    "    # ----------- Main Training Loop -----------\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_rewards = [0] * num_agents\n",
    "\n",
    "        while not done:\n",
    "            # Select actions\n",
    "            actions = []\n",
    "            for i, agent in enumerate(agents):\n",
    "                a = agent.qnet.get_action(obs[i].flatten(), epsilon=agent.epsilon)\n",
    "                actions.append(a)\n",
    "\n",
    "            # Step environment\n",
    "            next_obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            done = np.any(terminated) or np.any(truncated)\n",
    "\n",
    "            # Store transitions & train agents\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i].flatten(), actions[i], rewards[i], done, next_obs[i].flatten())\n",
    "                agent.update()\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Decay epsilon\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_MIN, agent.epsilon * agent.eps_decay)\n",
    "\n",
    "        # Logging\n",
    "        if ep % 10 == 0:\n",
    "            avg_rewards = [np.mean(rewards_history[i][-100:] + [total_rewards[i]]) for i in range(num_agents)]\n",
    "            print(f\"Episode {ep}, Avg rewards: {avg_rewards}\")\n",
    "            for i, avg in enumerate(avg_rewards):\n",
    "                wandb.log({f\"agent_{i}_avg_reward\": avg}, step=ep)\n",
    "                wandb.log({f\"agent_{i}_epsilon\": agents[i].epsilon}, step=ep)\n",
    "        \n",
    "        for i in range(num_agents):\n",
    "            wandb.log({f\"agent_{i}_total_reward\": total_rewards[i]}, step=ep)\n",
    "\n",
    "        \n",
    "        import os\n",
    "        if ep % 1000 == 0:\n",
    "            save_dir = \"./checkpoints\"\n",
    "            os.makedirs(save_dir, exist_ok=True)  #\n",
    "            for i, agent in enumerate(agents):\n",
    "                save_path = os.path.join(save_dir, f\"agent_{i}_ep{ep}.pt\")\n",
    "                torch.save(agent.qnet.state_dict(), save_path)\n",
    "                print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            rewards_history[i].append(total_rewards[i])\n",
    "\n",
    "    return agents, rewards_history\n",
    "\n",
    "# ---------------- Train ----------------\n",
    "agents, rewards_history = train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE)\n",
    "\n",
    "# ---------------- Plot learning curves ----------------\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i in range(env.unwrapped.n_agents):\n",
    "#     plt.plot(rewards_history[i], label=f\"Agent {i}\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.title(\"Learning Curves\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# ---------------- Cleanup ----------------\n",
    "wandb.finish()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7329f0e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Unable to save notebook session history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_0_avg_reward</td><td>▇▇▇██▄▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▂▂▂▂▂▂▂</td></tr><tr><td>agent_0_epsilon</td><td>██▇▇▇▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_0_loss</td><td>▅▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_0_total_reward</td><td>▁▁▆▇▆▅▇▁▅▁▁▁▅▁▄▁▄▁▁█▁▆▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_1_avg_reward</td><td>▇▇▇▇█▇▅▄▄▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▃▄▄▃▃▂▃▃▄▃▃</td></tr><tr><td>agent_1_epsilon</td><td>██▇▇▇▇▆▅▄▄▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_1_loss</td><td>▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_1_total_reward</td><td>▁▆▆▆▁▁▁▁▆▁▅▁▁▁▁▁▁▁▁▁▁▁▁▆▁▁▁▁▁▁▃▃▄▁▁▁▅▁▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_0_avg_reward</td><td>0.02881</td></tr><tr><td>agent_0_epsilon</td><td>0.05</td></tr><tr><td>agent_0_loss</td><td>0.00032</td></tr><tr><td>agent_0_total_reward</td><td>0</td></tr><tr><td>agent_1_avg_reward</td><td>0.04753</td></tr><tr><td>agent_1_epsilon</td><td>0.05</td></tr><tr><td>agent_1_loss</td><td>0.00032</td></tr><tr><td>agent_1_total_reward</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">strange-echo-2</strong> at: <a href='https://wandb.ai/marionapla/LBF_proves/runs/nvjevqcs' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves/runs/nvjevqcs</a><br> View project at: <a href='https://wandb.ai/marionapla/LBF_proves' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251031_161912-nvjevqcs\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\propietari\\Desktop\\TGF-MARL\\PROVES\\wandb\\run-20251031_164033-ewnv7rqo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marionapla/LBF_proves/runs/ewnv7rqo' target=\"_blank\">ineffable-treat-3</a></strong> to <a href='https://wandb.ai/marionapla/LBF_proves' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marionapla/LBF_proves' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marionapla/LBF_proves/runs/ewnv7rqo' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves/runs/ewnv7rqo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Agent 0: obs_dim=18, act_dim=6\n",
      "Initialized Agent 1: obs_dim=18, act_dim=6\n",
      "Burn-in buffer with random actions...\n",
      "Burn-in steps: 0/1000\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\propietari\\anaconda3\\lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:245: UserWarning: WARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burn-in complete.0/1000\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\propietari\\AppData\\Local\\Temp\\ipykernel_12480\\2407175908.py:124: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  dones = torch.BoolTensor(dones).to(self.qnet.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10, Avg rewards: [0.06877344877344878, 0.07833333333333334]\n",
      "Episode 20, Avg rewards: [0.07980339105339104, 0.07310897435897436]\n",
      "Episode 30, Avg rewards: [0.07354858104858104, 0.11241896991896991]\n",
      "Episode 40, Avg rewards: [0.07852453102453102, 0.10073287823287824]\n",
      "Episode 50, Avg rewards: [0.08984343434343434, 0.08652036852036851]\n",
      "Episode 60, Avg rewards: [0.08931397306397307, 0.09626697376697377]\n",
      "Episode 70, Avg rewards: [0.09191197691197692, 0.08763978085406658]\n",
      "Episode 80, Avg rewards: [0.09418380230880231, 0.08517038517038517]\n",
      "Episode 90, Avg rewards: [0.09615720699054031, 0.0840235073568407]\n",
      "Episode 100, Avg rewards: [0.09112481962481961, 0.08262115662115661]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep100.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep100.pt\n",
      "Episode 110, Avg rewards: [0.09311788321689311, 0.08639608466341141]\n",
      "Episode 120, Avg rewards: [0.09793086451502292, 0.08627549568143628]\n",
      "Episode 130, Avg rewards: [0.09848413412769848, 0.07918692968197918]\n",
      "Episode 140, Avg rewards: [0.09566706670667065, 0.08039705069408039]\n",
      "Episode 150, Avg rewards: [0.09742331375994742, 0.08372609238945873]\n",
      "Episode 160, Avg rewards: [0.08760483191176262, 0.07864201255290365]\n",
      "Episode 170, Avg rewards: [0.08898389838983899, 0.08318353813403316]\n",
      "Episode 180, Avg rewards: [0.08271648593430769, 0.08537318017516038]\n",
      "Episode 190, Avg rewards: [0.08354942637120855, 0.08447273298758445]\n",
      "Episode 200, Avg rewards: [0.08412698412698413, 0.08084236995128084]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep200.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep200.pt\n",
      "Episode 210, Avg rewards: [0.08078736445073079, 0.07616690240452617]\n",
      "Episode 220, Avg rewards: [0.07590759075907591, 0.07305909162344806]\n",
      "Episode 230, Avg rewards: [0.077993376260703, 0.06742102781706741]\n",
      "Episode 240, Avg rewards: [0.07128663415792129, 0.06501257268584]\n",
      "Episode 250, Avg rewards: [0.07003722350257004, 0.06667845355964168]\n",
      "Episode 260, Avg rewards: [0.07384831889782385, 0.06565692283514066]\n",
      "Episode 270, Avg rewards: [0.0743905159746744, 0.06007779349363508]\n",
      "Episode 280, Avg rewards: [0.0709015956540709, 0.05381895332390382]\n",
      "Episode 290, Avg rewards: [0.06344049240088845, 0.05440044004400441]\n",
      "Episode 300, Avg rewards: [0.0784019885505034, 0.05471475719000472]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep300.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep300.pt\n",
      "Episode 310, Avg rewards: [0.08321496984863323, 0.04951673738802452]\n",
      "Episode 320, Avg rewards: [0.08571379115933571, 0.052789564670752784]\n",
      "Episode 330, Avg rewards: [0.08160338011823161, 0.0484016258768734]\n",
      "Episode 340, Avg rewards: [0.079997642621405, 0.05575664709328075]\n",
      "Episode 350, Avg rewards: [0.07406883545497407, 0.056617804637606614]\n",
      "Episode 360, Avg rewards: [0.06825396825396826, 0.05817367451030818]\n",
      "Episode 370, Avg rewards: [0.06988841741316988, 0.057670876977807664]\n",
      "Episode 380, Avg rewards: [0.07361736173617361, 0.0613012400141113]\n",
      "Episode 390, Avg rewards: [0.07469425513979969, 0.06797333579511798]\n",
      "Episode 400, Avg rewards: [0.06675119892941675, 0.06542212829341543]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep400.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep400.pt\n",
      "Episode 410, Avg rewards: [0.06516782630644016, 0.07037262334292038]\n",
      "Episode 420, Avg rewards: [0.06673548307211674, 0.06872245832641873]\n",
      "Episode 430, Avg rewards: [0.07327720867324827, 0.06741732781336743]\n",
      "Episode 440, Avg rewards: [0.07212459341172212, 0.0712009204583462]\n",
      "Episode 450, Avg rewards: [0.07310290552864811, 0.080995114163431]\n",
      "Episode 460, Avg rewards: [0.07722831806990223, 0.0774904779855275]\n",
      "Episode 470, Avg rewards: [0.07288288352644788, 0.07690888136432691]\n",
      "Episode 480, Avg rewards: [0.06626615042456627, 0.07602879335552604]\n",
      "Episode 490, Avg rewards: [0.06997473556879498, 0.07035072554874534]\n",
      "Episode 500, Avg rewards: [0.07502607403597503, 0.07798601288700299]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep500.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep500.pt\n",
      "Episode 510, Avg rewards: [0.07475176089037475, 0.07935364965067934]\n",
      "Episode 520, Avg rewards: [0.06958517280299457, 0.08107132141785607]\n",
      "Episode 530, Avg rewards: [0.0658998042661409, 0.08241502721700741]\n",
      "Episode 540, Avg rewards: [0.07189540382609688, 0.08058805880588057]\n",
      "Episode 550, Avg rewards: [0.08006907833640507, 0.06311988341691312]\n",
      "Episode 560, Avg rewards: [0.0813067020987813, 0.0594895203806095]\n",
      "Episode 570, Avg rewards: [0.07478855028359979, 0.06113968539711114]\n",
      "Episode 580, Avg rewards: [0.07567256725672568, 0.06774034546311773]\n",
      "Episode 590, Avg rewards: [0.07636799394225137, 0.0641924906776392]\n",
      "Episode 600, Avg rewards: [0.0681878902175932, 0.06445573128741447]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep600.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep600.pt\n",
      "Episode 610, Avg rewards: [0.07348341977054848, 0.06532760418899033]\n",
      "Episode 620, Avg rewards: [0.07293336476504793, 0.06294236566513793]\n",
      "Episode 630, Avg rewards: [0.07084708470847086, 0.07281978197819783]\n",
      "Episode 640, Avg rewards: [0.06654093980826654, 0.07360164587887359]\n",
      "Episode 650, Avg rewards: [0.06424642464246424, 0.0767958938751018]\n",
      "Episode 660, Avg rewards: [0.059708470847084714, 0.08481098109810983]\n",
      "Episode 670, Avg rewards: [0.05847084708470847, 0.08542389953281043]\n",
      "Episode 680, Avg rewards: [0.057987584472732986, 0.07854821196405355]\n",
      "Episode 690, Avg rewards: [0.0548019087623048, 0.08267362450530767]\n",
      "Episode 700, Avg rewards: [0.05974061691883474, 0.078002085922878]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep700.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep700.pt\n",
      "Episode 710, Avg rewards: [0.053713585644278715, 0.06907940794079406]\n",
      "Episode 720, Avg rewards: [0.057732064415232744, 0.07172717271727173]\n",
      "Episode 730, Avg rewards: [0.05630192140093131, 0.06298129812981297]\n",
      "Episode 740, Avg rewards: [0.06190462452838691, 0.05752003771805752]\n",
      "Episode 750, Avg rewards: [0.05245939429107746, 0.05416470218450418]\n",
      "Episode 760, Avg rewards: [0.05585401946788085, 0.045678139242495676]\n",
      "Episode 770, Avg rewards: [0.05742560519788242, 0.04567813924249568]\n",
      "Episode 780, Avg rewards: [0.05785386230930784, 0.04561527581329561]\n",
      "Episode 790, Avg rewards: [0.06507458438151507, 0.04814552883859814]\n",
      "Episode 800, Avg rewards: [0.059299006823759305, 0.04608282256797108]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep800.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep800.pt\n",
      "Episode 810, Avg rewards: [0.05825390231330825, 0.048157315731573155]\n",
      "Episode 820, Avg rewards: [0.053009586672953, 0.04075907590759076]\n",
      "Episode 830, Avg rewards: [0.053909676681953914, 0.042998585572842994]\n",
      "Episode 840, Avg rewards: [0.04279070764219279, 0.03818953323903818]\n",
      "Episode 850, Avg rewards: [0.04459088766019458, 0.03443737230865943]\n",
      "Episode 860, Avg rewards: [0.05185161373280185, 0.0388967468175389]\n",
      "Episode 870, Avg rewards: [0.05444473018730443, 0.051916125678501915]\n",
      "Episode 880, Avg rewards: [0.06215728715728716, 0.05947973918270948]\n",
      "Episode 890, Avg rewards: [0.06855757004271856, 0.06423378601596423]\n",
      "Episode 900, Avg rewards: [0.06649486377209149, 0.06641829018066642]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep900.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep900.pt\n",
      "Episode 910, Avg rewards: [0.06592909290929093, 0.06698406104346699]\n",
      "Episode 920, Avg rewards: [0.07139035332104637, 0.07462232486984963]\n",
      "Episode 930, Avg rewards: [0.07124462446244624, 0.08447616739695947]\n",
      "Episode 940, Avg rewards: [0.07867036703670367, 0.10129213470797628]\n",
      "Episode 950, Avg rewards: [0.07852035203520352, 0.1088828937838839]\n",
      "Episode 960, Avg rewards: [0.06786500078579286, 0.1064001455090564]\n",
      "Episode 970, Avg rewards: [0.06252160930378751, 0.10072400097152572]\n",
      "Episode 980, Avg rewards: [0.05252632406097753, 0.09988284542739986]\n",
      "Episode 990, Avg rewards: [0.04116054462589116, 0.09985177089137483]\n",
      "Episode 1000, Avg rewards: [0.04828375694712328, 0.09403690369036906]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1000.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1000.pt\n",
      "Episode 1010, Avg rewards: [0.04459053048161958, 0.09348684868486849]\n",
      "Episode 1020, Avg rewards: [0.04101517294586602, 0.09217421742174217]\n",
      "Episode 1030, Avg rewards: [0.044574814624319566, 0.07845427399882846]\n",
      "Episode 1040, Avg rewards: [0.039624319574814625, 0.0643991596961894]\n",
      "Episode 1050, Avg rewards: [0.04038593419781538, 0.05624262975748125]\n",
      "Episode 1060, Avg rewards: [0.0418003613548168, 0.057071284051482075]\n",
      "Episode 1070, Avg rewards: [0.04321478851181821, 0.04725280220329726]\n",
      "Episode 1080, Avg rewards: [0.044939961029069934, 0.04308810001879309]\n",
      "Episode 1090, Avg rewards: [0.04532428517577032, 0.03698999020781199]\n",
      "Episode 1100, Avg rewards: [0.041281380885341286, 0.04095038624741595]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1100.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1100.pt\n",
      "Episode 1110, Avg rewards: [0.04272331079261773, 0.04273806501529274]\n",
      "Episode 1120, Avg rewards: [0.04373769794561874, 0.03616097873523615]\n",
      "Episode 1130, Avg rewards: [0.04401272544836901, 0.04095824417606596]\n",
      "Episode 1140, Avg rewards: [0.040123050766615126, 0.048636649379223636]\n",
      "Episode 1150, Avg rewards: [0.044602070097119605, 0.05846691812038347]\n",
      "Episode 1160, Avg rewards: [0.05016680239452517, 0.06233301901618733]\n",
      "Episode 1170, Avg rewards: [0.05219021902190219, 0.06490649064906491]\n",
      "Episode 1180, Avg rewards: [0.05460224593887961, 0.06091859185918591]\n",
      "Episode 1190, Avg rewards: [0.05951345134513452, 0.05885588558855886]\n",
      "Episode 1200, Avg rewards: [0.05148264826482649, 0.05588558855885588]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1200.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1200.pt\n",
      "Episode 1210, Avg rewards: [0.04784049833554785, 0.06266305201948766]\n",
      "Episode 1220, Avg rewards: [0.04352578114954352, 0.06327204148986326]\n",
      "Episode 1230, Avg rewards: [0.03747517608903748, 0.058930535910733925]\n",
      "Episode 1240, Avg rewards: [0.04251996628234252, 0.049387081565299376]\n",
      "Episode 1250, Avg rewards: [0.041282342519966285, 0.04005186232909005]\n",
      "Episode 1260, Avg rewards: [0.030535910733930534, 0.0396007457888646]\n",
      "Episode 1270, Avg rewards: [0.03016265912305516, 0.03892925006786393]\n",
      "Episode 1280, Avg rewards: [0.032225365393682225, 0.039616818824739615]\n",
      "Episode 1290, Avg rewards: [0.03108203677510608, 0.040756218478990754]\n",
      "Episode 1300, Avg rewards: [0.03273220179160773, 0.041003743231466]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1300.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1300.pt\n",
      "Episode 1310, Avg rewards: [0.034382366808109376, 0.044869844127269874]\n",
      "Episode 1320, Avg rewards: [0.041211621162116206, 0.037895932450387894]\n",
      "Episode 1330, Avg rewards: [0.04121162116211621, 0.04041046961839041]\n",
      "Episode 1340, Avg rewards: [0.03781699598531281, 0.037644478733587644]\n",
      "Episode 1350, Avg rewards: [0.04672395811009672, 0.037149429228637156]\n",
      "Episode 1360, Avg rewards: [0.04813838526709813, 0.03163637792350664]\n",
      "Episode 1370, Avg rewards: [0.053217327227228224, 0.024194562313374197]\n",
      "Episode 1380, Avg rewards: [0.054042409735479045, 0.023231966053748228]\n",
      "Episode 1390, Avg rewards: [0.04711564563049712, 0.023506993556498507]\n",
      "Episode 1400, Avg rewards: [0.04458539260519458, 0.02226936979412227]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1400.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1400.pt\n",
      "Episode 1410, Avg rewards: [0.04458539260519458, 0.014926135470689926]\n",
      "Episode 1420, Avg rewards: [0.03955631826918955, 0.01782964010686783]\n",
      "Episode 1430, Avg rewards: [0.04097074542619097, 0.02002986012887003]\n",
      "Episode 1440, Avg rewards: [0.04318668130549319, 0.023753309396873753]\n",
      "Episode 1450, Avg rewards: [0.03804759596838805, 0.033222113420133224]\n",
      "Episode 1460, Avg rewards: [0.03925771698048926, 0.0400978009888901]\n",
      "Episode 1470, Avg rewards: [0.03490563342048491, 0.04207799900869208]\n",
      "Episode 1480, Avg rewards: [0.0378052090923378, 0.0429030815169429]\n",
      "Episode 1490, Avg rewards: [0.037019416227337024, 0.04297380287479297]\n",
      "Episode 1500, Avg rewards: [0.04265748003371766, 0.04462396789129462]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1500.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1500.pt\n",
      "Episode 1510, Avg rewards: [0.04403261754746903, 0.04503650914542003]\n",
      "Episode 1520, Avg rewards: [0.0447076850542197, 0.0426987753720427]\n",
      "Episode 1530, Avg rewards: [0.043293257897218294, 0.041032894498241025]\n",
      "Episode 1540, Avg rewards: [0.04048797736916549, 0.0358950180732359]\n",
      "Episode 1550, Avg rewards: [0.04356828539996857, 0.02303158887317303]\n",
      "Episode 1560, Avg rewards: [0.04204384724186704, 0.021955052648121955]\n",
      "Episode 1570, Avg rewards: [0.03921499292786421, 0.027755632706127756]\n",
      "Episode 1580, Avg rewards: [0.03796558227251297, 0.025225379680825225]\n",
      "Episode 1590, Avg rewards: [0.04141914191419141, 0.02126498364122126]\n",
      "Episode 1600, Avg rewards: [0.03446094609460946, 0.028054233994828058]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1600.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1600.pt\n",
      "Episode 1610, Avg rewards: [0.03850777934936351, 0.02946866115182947]\n",
      "Episode 1620, Avg rewards: [0.03886138613861386, 0.03364907919363365]\n",
      "Episode 1630, Avg rewards: [0.04066156615661566, 0.03183461203263184]\n",
      "Episode 1640, Avg rewards: [0.0418009658108668, 0.03668688297401169]\n",
      "Episode 1650, Avg rewards: [0.04129091480576628, 0.03866708099381367]\n",
      "Episode 1660, Avg rewards: [0.0500917948937751, 0.03772841569871273]\n",
      "Episode 1670, Avg rewards: [0.04934922063634934, 0.03792343520066293]\n",
      "Episode 1680, Avg rewards: [0.050642207077850644, 0.032972940151157974]\n",
      "Episode 1690, Avg rewards: [0.04999392796422499, 0.035801794465160804]\n",
      "Episode 1700, Avg rewards: [0.05387181575300387, 0.027362379095052364]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1700.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1700.pt\n",
      "Episode 1710, Avg rewards: [0.046799679967996805, 0.025397896932550398]\n",
      "Episode 1720, Avg rewards: [0.04848127669909848, 0.027335590701927336]\n",
      "Episode 1730, Avg rewards: [0.049156344205849155, 0.025205377680625203]\n",
      "Episode 1740, Avg rewards: [0.045541697026845546, 0.024596388210249596]\n",
      "Episode 1750, Avg rewards: [0.043741517008843746, 0.0212017630334462]\n",
      "Episode 1760, Avg rewards: [0.04000114297144, 0.01832147500464332]\n",
      "Episode 1770, Avg rewards: [0.03522745131656023, 0.023735587844498734]\n",
      "Episode 1780, Avg rewards: [0.03123919534810624, 0.024835697855499834]\n",
      "Episode 1790, Avg rewards: [0.03202498821310702, 0.02672160073150172]\n",
      "Episode 1800, Avg rewards: [0.025424328147100425, 0.0312006200620062]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1800.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1800.pt\n",
      "Episode 1810, Avg rewards: [0.02683875530410184, 0.04377330590201878]\n",
      "Episode 1820, Avg rewards: [0.024292786421499294, 0.048888103096023885]\n",
      "Episode 1830, Avg rewards: [0.0224976069035475, 0.048338048090523326]\n",
      "Episode 1840, Avg rewards: [0.02712985584272713, 0.0537993085022788]\n",
      "Episode 1850, Avg rewards: [0.02795172374380295, 0.0596927549897847]\n",
      "Episode 1860, Avg rewards: [0.03077700627205578, 0.063994970925664]\n",
      "Episode 1870, Avg rewards: [0.04298822739416799, 0.06122112211221122]\n",
      "Episode 1880, Avg rewards: [0.04133806237766634, 0.0621012101210121]\n",
      "Episode 1890, Avg rewards: [0.03666259483091166, 0.06210121012101209]\n",
      "Episode 1900, Avg rewards: [0.039000328604289, 0.06106003457488605]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep1900.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep1900.pt\n",
      "Episode 1910, Avg rewards: [0.041711313988541714, 0.05127762776277627]\n",
      "Episode 1920, Avg rewards: [0.041947051848041955, 0.04706970697069707]\n",
      "Episode 1930, Avg rewards: [0.03906676381923907, 0.05899018473275899]\n",
      "Episode 1940, Avg rewards: [0.035424613889960425, 0.0539021759318789]\n",
      "Episode 1950, Avg rewards: [0.030972382952580976, 0.054797979797979804]\n",
      "Episode 1960, Avg rewards: [0.028647150429328645, 0.05049576386210049]\n",
      "Episode 1970, Avg rewards: [0.018836169331218837, 0.04840555484119841]\n",
      "Episode 1980, Avg rewards: [0.013555641278413556, 0.05013822810852513]\n",
      "Episode 1990, Avg rewards: [0.02355092652122355, 0.05152908147957653]\n",
      "Episode 2000, Avg rewards: [0.021625734001971618, 0.059147343305759145]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2000.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2000.pt\n",
      "Episode 2010, Avg rewards: [0.024493877959224492, 0.05667138142385667]\n",
      "Episode 2020, Avg rewards: [0.026294057977226292, 0.05591702027345592]\n",
      "Episode 2030, Avg rewards: [0.031024531024531024, 0.0527895646707528]\n",
      "Episode 2040, Avg rewards: [0.03564499307073564, 0.048526638378123516]\n",
      "Episode 2050, Avg rewards: [0.03927535610703928, 0.04032296086751532]\n",
      "Episode 2060, Avg rewards: [0.03912534110553912, 0.04197312588401697]\n",
      "Episode 2070, Avg rewards: [0.04177739202491678, 0.03729765833726229]\n",
      "Episode 2080, Avg rewards: [0.043663294900918664, 0.03499921420713499]\n",
      "Episode 2090, Avg rewards: [0.037557684339862556, 0.03087380166588087]\n",
      "Episode 2100, Avg rewards: [0.041565227951366565, 0.021727172717271724]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2100.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2100.pt\n",
      "Episode 2110, Avg rewards: [0.041140899804266134, 0.02031274556027031]\n",
      "Episode 2120, Avg rewards: [0.04475876159044476, 0.01984126984126984]\n",
      "Episode 2130, Avg rewards: [0.041128398554141125, 0.012533396196762532]\n",
      "Episode 2140, Avg rewards: [0.03732516108753733, 0.01585337105139085]\n",
      "Episode 2150, Avg rewards: [0.03966289486091467, 0.019153701084394154]\n",
      "Episode 2160, Avg rewards: [0.03727015558698727, 0.019153701084394154]\n",
      "Episode 2170, Avg rewards: [0.037093352192362095, 0.021903976111896907]\n",
      "Episode 2180, Avg rewards: [0.037368379695112364, 0.0219039761118969]\n",
      "Episode 2190, Avg rewards: [0.03875923306616376, 0.02441851327989942]\n",
      "Episode 2200, Avg rewards: [0.0397021845041647, 0.021766462360521766]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2200.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2200.pt\n",
      "Episode 2210, Avg rewards: [0.03478312116925978, 0.026041889903276043]\n",
      "Episode 2220, Avg rewards: [0.02804494735187804, 0.025193233609075192]\n",
      "Episode 2230, Avg rewards: [0.02928257111425428, 0.025193233609075192]\n",
      "Episode 2240, Avg rewards: [0.03156994270855657, 0.026643021445001646]\n",
      "Episode 2250, Avg rewards: [0.03229680110868229, 0.024992856428499993]\n",
      "Episode 2260, Avg rewards: [0.030646636092180643, 0.02169252639549669]\n",
      "Episode 2270, Avg rewards: [0.029971568585429972, 0.017292086351492292]\n",
      "Episode 2280, Avg rewards: [0.029696541082679693, 0.019492306373494493]\n",
      "Episode 2290, Avg rewards: [0.02689126055462689, 0.017901075821867903]\n",
      "Episode 2300, Avg rewards: [0.026160473190176156, 0.018077879216493075]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2300.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2300.pt\n",
      "Episode 2310, Avg rewards: [0.02873001585872873, 0.013802451673738801]\n",
      "Episode 2320, Avg rewards: [0.0344005829154344, 0.01272234366293772]\n",
      "Episode 2330, Avg rewards: [0.035913234180560916, 0.012722343662937721]\n",
      "Episode 2340, Avg rewards: [0.0331936765105082, 0.006852470961381853]\n",
      "Episode 2350, Avg rewards: [0.03493813667080994, 0.006852470961381853]\n",
      "Episode 2360, Avg rewards: [0.033276184761333275, 0.010247096138185247]\n",
      "Episode 2370, Avg rewards: [0.03289043190033289, 0.013075950452188076]\n",
      "Episode 2380, Avg rewards: [0.03157029988713157, 0.01427035560698927]\n",
      "Episode 2390, Avg rewards: [0.03579000757218579, 0.013032731844613033]\n",
      "Episode 2400, Avg rewards: [0.03528317117426028, 0.015331175974740333]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2400.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2400.pt\n",
      "Episode 2410, Avg rewards: [0.03289043190033289, 0.02248189104624748]\n",
      "Episode 2420, Avg rewards: [0.024925349677824924, 0.024682111068249683]\n",
      "Episode 2430, Avg rewards: [0.022175074650322174, 0.026257268584001257]\n",
      "Episode 2440, Avg rewards: [0.0215000071435715, 0.029321860757504318]\n",
      "Episode 2450, Avg rewards: [0.019755546983269753, 0.03427235580700927]\n",
      "Episode 2460, Avg rewards: [0.02404597602617405, 0.034068049662109065]\n",
      "Episode 2470, Avg rewards: [0.02443172888717443, 0.037879145057362874]\n",
      "Episode 2480, Avg rewards: [0.02425171088537425, 0.043120383466918114]\n",
      "Episode 2490, Avg rewards: [0.02168216821682168, 0.050971168545425966]\n",
      "Episode 2500, Avg rewards: [0.018794379437943796, 0.06268734016258769]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2500.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2500.pt\n",
      "Episode 2510, Avg rewards: [0.026766248053376768, 0.057461817610332463]\n",
      "Episode 2520, Avg rewards: [0.03309188061663309, 0.05889196062463389]\n",
      "Episode 2530, Avg rewards: [0.034329504379009335, 0.06065999457088567]\n",
      "Episode 2540, Avg rewards: [0.03252932436100753, 0.06042425671138542]\n",
      "Episode 2550, Avg rewards: [0.034839555384109835, 0.05671138542425671]\n",
      "Episode 2560, Avg rewards: [0.029559027331304555, 0.061536153615361536]\n",
      "Episode 2570, Avg rewards: [0.02775884731330276, 0.05613382766848113]\n",
      "Episode 2580, Avg rewards: [0.029023259468804023, 0.0474979640821225]\n",
      "Episode 2590, Avg rewards: [0.02737309445230237, 0.04267248153386768]\n",
      "Episode 2600, Avg rewards: [0.02737309445230237, 0.036649379223636654]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2600.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2600.pt\n",
      "Episode 2610, Avg rewards: [0.02014380009429514, 0.029773691654879773]\n",
      "Episode 2620, Avg rewards: [0.01663130598774163, 0.03191890617633192]\n",
      "Episode 2630, Avg rewards: [0.01704384724186704, 0.02909005186232909]\n",
      "Episode 2640, Avg rewards: [0.01704384724186704, 0.024846770391324845]\n",
      "Episode 2650, Avg rewards: [0.015063649222065063, 0.026084394153701083]\n",
      "Episode 2660, Avg rewards: [0.01954871311306955, 0.019592536176694594]\n",
      "Episode 2670, Avg rewards: [0.023261584400198262, 0.020830159939070832]\n",
      "Episode 2680, Avg rewards: [0.020196992226695194, 0.020830159939070832]\n",
      "Episode 2690, Avg rewards: [0.021352107738246355, 0.016704747397816706]\n",
      "Episode 2700, Avg rewards: [0.02135210773824635, 0.01224930185326225]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2700.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2700.pt\n",
      "Episode 2710, Avg rewards: [0.02135210773824635, 0.01224930185326225]\n",
      "Episode 2720, Avg rewards: [0.023049420326648046, 0.008123889312008123]\n",
      "Episode 2730, Avg rewards: [0.02304942032664805, 0.008885503935008884]\n",
      "Episode 2740, Avg rewards: [0.019749090293644748, 0.008885503935008886]\n",
      "Episode 2750, Avg rewards: [0.01776889227384277, 0.008390454430058389]\n",
      "Episode 2760, Avg rewards: [0.015264026402640263, 0.006867225184056867]\n",
      "Episode 2770, Avg rewards: [0.015264026402640263, 0.004391977659304392]\n",
      "Episode 2780, Avg rewards: [0.017244224422442243, 0.004391977659304392]\n",
      "Episode 2790, Avg rewards: [0.01773927392739274, 0.004391977659304392]\n",
      "Episode 2800, Avg rewards: [0.019539453945394542, 0.006372175679106373]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2800.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2800.pt\n",
      "Episode 2810, Avg rewards: [0.026015101510151017, 0.008847423203858848]\n",
      "Episode 2820, Avg rewards: [0.030140514051405144, 0.008847423203858848]\n",
      "Episode 2830, Avg rewards: [0.02838033803380338, 0.009323432343234322]\n",
      "Episode 2840, Avg rewards: [0.02838033803380338, 0.009087694483734088]\n",
      "Episode 2850, Avg rewards: [0.02838033803380338, 0.009087694483734088]\n",
      "Episode 2860, Avg rewards: [0.0264001400140014, 0.007107496463932108]\n",
      "Episode 2870, Avg rewards: [0.022687268726872687, 0.007107496463932108]\n",
      "Episode 2880, Avg rewards: [0.020707070707070705, 0.007107496463932108]\n",
      "Episode 2890, Avg rewards: [0.023771662880573766, 0.009307716485934307]\n",
      "Episode 2900, Avg rewards: [0.019496235337819495, 0.009802765990884803]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep2900.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep2900.pt\n",
      "Episode 2910, Avg rewards: [0.013020587773063018, 0.012002986012887002]\n",
      "Episode 2920, Avg rewards: [0.006914977212006914, 0.012002986012887002]\n",
      "Episode 2930, Avg rewards: [0.005614847199005615, 0.01636020744931636]\n",
      "Episode 2940, Avg rewards: [0.008915177232008914, 0.014945780292314945]\n",
      "Episode 2950, Avg rewards: [0.010895375251810894, 0.016595945308816595]\n",
      "Episode 2960, Avg rewards: [0.012875573271612874, 0.018576143328618573]\n",
      "Episode 2970, Avg rewards: [0.016765247953366764, 0.021228194247996223]\n",
      "Episode 2980, Avg rewards: [0.016765247953366764, 0.03101524438158101]\n",
      "Episode 2990, Avg rewards: [0.01315060077436315, 0.03695583844098696]\n",
      "Episode 3000, Avg rewards: [0.01315060077436315, 0.03716014458588716]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3000.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3000.pt\n",
      "Episode 3010, Avg rewards: [0.01315060077436315, 0.032484677039132485]\n",
      "Episode 3020, Avg rewards: [0.01315060077436315, 0.03554926921263555]\n",
      "Episode 3030, Avg rewards: [0.01225051076536225, 0.035152443815810154]\n",
      "Episode 3040, Avg rewards: [0.00895018073235895, 0.04293179317931793]\n",
      "Episode 3050, Avg rewards: [0.011507936507936507, 0.046974697469746966]\n",
      "Episode 3060, Avg rewards: [0.009527738488134527, 0.04838912462674838]\n",
      "Episode 3070, Avg rewards: [0.007052490963382052, 0.048389124626748395]\n",
      "Episode 3080, Avg rewards: [0.005638063806380638, 0.04506129184347006]\n",
      "Episode 3090, Avg rewards: [0.0065181518151815184, 0.043752946723243744]\n",
      "Episode 3100, Avg rewards: [0.0065181518151815184, 0.04331361707599331]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3100.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3100.pt\n",
      "Episode 3110, Avg rewards: [0.0065181518151815184, 0.04637820924949638]\n",
      "Episode 3120, Avg rewards: [0.0065181518151815184, 0.05118726158330118]\n",
      "Episode 3130, Avg rewards: [0.008168316831683169, 0.0482994728044233]\n",
      "Episode 3140, Avg rewards: [0.010148514851485147, 0.04470054148271971]\n",
      "Episode 3150, Avg rewards: [0.00759075907590759, 0.04478304973354478]\n",
      "Episode 3160, Avg rewards: [0.009790979097909791, 0.04138842455674139]\n",
      "Episode 3170, Avg rewards: [0.009790979097909791, 0.0451012958438701]\n",
      "Episode 3180, Avg rewards: [0.011441144114411442, 0.04651572300087152]\n",
      "Episode 3190, Avg rewards: [0.011441144114411442, 0.0484016258768734]\n",
      "Episode 3200, Avg rewards: [0.01409319503378909, 0.04686075750432186]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3200.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3200.pt\n",
      "Episode 3210, Avg rewards: [0.019043690083294047, 0.043796165330818794]\n",
      "Episode 3220, Avg rewards: [0.020458117240295455, 0.04181596731101681]\n",
      "Episode 3230, Avg rewards: [0.020222379380795224, 0.03842134213421342]\n",
      "Episode 3240, Avg rewards: [0.020222379380795224, 0.03506600660066007]\n",
      "Episode 3250, Avg rewards: [0.021636806537796635, 0.03723479490806223]\n",
      "Episode 3260, Avg rewards: [0.020674210278170672, 0.04209528095666709]\n",
      "Episode 3270, Avg rewards: [0.02714793457367715, 0.047028851236772036]\n",
      "Episode 3280, Avg rewards: [0.02856236173067856, 0.04585016193927085]\n",
      "Episode 3290, Avg rewards: [0.0302950349980053, 0.03939094458896439]\n",
      "Episode 3300, Avg rewards: [0.03108082786300608, 0.04031425120534032]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3300.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3300.pt\n",
      "Episode 3310, Avg rewards: [0.02736795657587737, 0.041124332213441125]\n",
      "Episode 3320, Avg rewards: [0.02595352941887595, 0.039533101661814535]\n",
      "Episode 3330, Avg rewards: [0.02835805558577836, 0.0392030686585142]\n",
      "Episode 3340, Avg rewards: [0.03198841862208199, 0.04651094230302151]\n",
      "Episode 3350, Avg rewards: [0.028263760441978265, 0.0437920989901188]\n",
      "Episode 3360, Avg rewards: [0.031976631729106975, 0.04608732851307109]\n",
      "Episode 3370, Avg rewards: [0.025502907433600504, 0.04702363093452203]\n",
      "Episode 3380, Avg rewards: [0.022438315260097437, 0.041743102881716745]\n",
      "Episode 3390, Avg rewards: [0.020525623990970524, 0.042568185389967574]\n",
      "Episode 3400, Avg rewards: [0.017087780206592084, 0.04776227622762277]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3400.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3400.pt\n",
      "Episode 3410, Avg rewards: [0.015850156444215845, 0.0542915005786293]\n",
      "Episode 3420, Avg rewards: [0.015850156444215845, 0.05305387681625306]\n",
      "Episode 3430, Avg rewards: [0.015425828297115423, 0.056024173845956024]\n",
      "Episode 3440, Avg rewards: [0.011795465260811794, 0.05826368351120826]\n",
      "Episode 3450, Avg rewards: [0.011795465260811794, 0.05162373380195162]\n",
      "Episode 3460, Avg rewards: [0.005194805194805194, 0.0491984912776992]\n",
      "Episode 3470, Avg rewards: [0.005194805194805194, 0.04120697784064121]\n",
      "Episode 3480, Avg rewards: [0.005194805194805194, 0.04103017444601603]\n",
      "Episode 3490, Avg rewards: [0.0066092323518066095, 0.04236209335219237]\n",
      "Episode 3500, Avg rewards: [0.004809052333804809, 0.037659123055162655]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3500.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3500.pt\n",
      "Episode 3510, Avg rewards: [0.009209492377809209, 0.0298011944051548]\n",
      "Episode 3520, Avg rewards: [0.014710042432814709, 0.03624469589816124]\n",
      "Episode 3530, Avg rewards: [0.01438000942951438, 0.036492220650636495]\n",
      "Episode 3540, Avg rewards: [0.01801037246581801, 0.03319189061763319]\n",
      "Episode 3550, Avg rewards: [0.021133899104196132, 0.03806023459488806]\n",
      "Episode 3560, Avg rewards: [0.021133899104196132, 0.03961217550326461]\n",
      "Episode 3570, Avg rewards: [0.021133899104196132, 0.03961217550326461]\n",
      "Episode 3580, Avg rewards: [0.02525931164545026, 0.043325046790393326]\n",
      "Episode 3590, Avg rewards: [0.028245324532453243, 0.04886881545297386]\n",
      "Episode 3600, Avg rewards: [0.028245324532453243, 0.05332426099752833]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3600.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3600.pt\n",
      "Episode 3610, Avg rewards: [0.027734559170202733, 0.053764305001928754]\n",
      "Episode 3620, Avg rewards: [0.03144350149300644, 0.05402312209242903]\n",
      "Episode 3630, Avg rewards: [0.032103567499607105, 0.049705190299249705]\n",
      "Episode 3640, Avg rewards: [0.028473204463303474, 0.0507968379255508]\n",
      "Episode 3650, Avg rewards: [0.025585415684425582, 0.04766902514427267]\n",
      "Episode 3660, Avg rewards: [0.027785635706427784, 0.047217194246897215]\n",
      "Episode 3670, Avg rewards: [0.03226465503693226, 0.05463386998040464]\n",
      "Episode 3680, Avg rewards: [0.02978940751217979, 0.05133353994740133]\n",
      "Episode 3690, Avg rewards: [0.02883931250267884, 0.048155722165623166]\n",
      "Episode 3700, Avg rewards: [0.027189147486177186, 0.04452535912931952]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3700.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3700.pt\n",
      "Episode 3710, Avg rewards: [0.027354163987827355, 0.04023493008641523]\n",
      "Episode 3720, Avg rewards: [0.023425199662823425, 0.037297273683412305]\n",
      "Episode 3730, Avg rewards: [0.02340948380552341, 0.040817625718615816]\n",
      "Episode 3740, Avg rewards: [0.028607503607503606, 0.048495426465723496]\n",
      "Episode 3750, Avg rewards: [0.030670209878130673, 0.04053141577894054]\n",
      "Episode 3760, Avg rewards: [0.02846998985612847, 0.038095457897438095]\n",
      "Episode 3770, Avg rewards: [0.02399097052562399, 0.031916405926306915]\n",
      "Episode 3780, Avg rewards: [0.02234080550912234, 0.03026624090980527]\n",
      "Episode 3790, Avg rewards: [0.022740845513122738, 0.028136027888503137]\n",
      "Episode 3800, Avg rewards: [0.02472104353292472, 0.028136027888503133]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3800.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3800.pt\n",
      "Episode 3810, Avg rewards: [0.022080779506522083, 0.03241074107410741]\n",
      "Episode 3820, Avg rewards: [0.0201005814867201, 0.029196133899104193]\n",
      "Episode 3830, Avg rewards: [0.013091309130913092, 0.02886610089580387]\n",
      "Episode 3840, Avg rewards: [0.012348734873487349, 0.02184111268269684]\n",
      "Episode 3850, Avg rewards: [0.008635863586358636, 0.02593973683082594]\n",
      "Episode 3860, Avg rewards: [0.011149191842261149, 0.029805837726629803]\n",
      "Episode 3870, Avg rewards: [0.015533916028965537, 0.03508636577943509]\n",
      "Episode 3880, Avg rewards: [0.01875173781114375, 0.03830418756161331]\n",
      "Episode 3890, Avg rewards: [0.01655151778914155, 0.04622377072872123]\n",
      "Episode 3900, Avg rewards: [0.01853171580894353, 0.048699018253473704]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep3900.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep3900.pt\n",
      "Episode 3910, Avg rewards: [0.022162078845247162, 0.052549403291977555]\n",
      "Episode 3920, Avg rewards: [0.026122474884851123, 0.05792422648858292]\n",
      "Episode 3930, Avg rewards: [0.026122474884851123, 0.05487142120805487]\n",
      "Episode 3940, Avg rewards: [0.021667029340296663, 0.053456994051053454]\n",
      "Episode 3950, Avg rewards: [0.029917854422804917, 0.050772797059925774]\n",
      "Episode 3960, Avg rewards: [0.03178925035360679, 0.04690669616412191]\n",
      "Episode 3970, Avg rewards: [0.028394625176803395, 0.045201525647070195]\n",
      "Episode 3980, Avg rewards: [0.028064592173503065, 0.041983703864891996]\n",
      "Episode 3990, Avg rewards: [0.030539839698255543, 0.034889203206034884]\n",
      "Episode 4000, Avg rewards: [0.031034889203206035, 0.032413955681282414]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4000.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4000.pt\n",
      "Episode 4010, Avg rewards: [0.030484834197705486, 0.026017601760176017]\n",
      "Episode 4020, Avg rewards: [0.022564042118497562, 0.022057205720572057]\n",
      "Episode 4030, Avg rewards: [0.029329718686154332, 0.024944994499449945]\n",
      "Episode 4040, Avg rewards: [0.029329718686154325, 0.027585258525852585]\n",
      "Episode 4050, Avg rewards: [0.024693540782649695, 0.024756404211849752]\n",
      "Episode 4060, Avg rewards: [0.022713342762847713, 0.02782099638535282]\n",
      "Episode 4070, Avg rewards: [0.019318717586044316, 0.02876037603760376]\n",
      "Episode 4080, Avg rewards: [0.018906176331918906, 0.029860486048604862]\n",
      "Episode 4090, Avg rewards: [0.019648750589344646, 0.027620976383352622]\n",
      "Episode 4100, Avg rewards: [0.01841112682696841, 0.028446058891603447]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4100.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4100.pt\n",
      "Episode 4110, Avg rewards: [0.01731101681596731, 0.027267369594102264]\n",
      "Episode 4120, Avg rewards: [0.01731101681596731, 0.031227765633706224]\n",
      "Episode 4130, Avg rewards: [0.011782964010686783, 0.029192562113354192]\n",
      "Episode 4140, Avg rewards: [0.013763162030488763, 0.02944008686582944]\n",
      "Episode 4150, Avg rewards: [0.011798679867986798, 0.03356549940708356]\n",
      "Episode 4160, Avg rewards: [0.011798679867986798, 0.035368894032260366]\n",
      "Episode 4170, Avg rewards: [0.014273927392739274, 0.040358678725015355]\n",
      "Episode 4180, Avg rewards: [0.011798679867986798, 0.044240138299544235]\n",
      "Episode 4190, Avg rewards: [0.01134171109418634, 0.04880166588087379]\n",
      "Episode 4200, Avg rewards: [0.011754252348311754, 0.04750510765362251]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4200.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4200.pt\n",
      "Episode 4210, Avg rewards: [0.011754252348311754, 0.05139478233537639]\n",
      "Episode 4220, Avg rewards: [0.011754252348311754, 0.049178846456074184]\n",
      "Episode 4230, Avg rewards: [0.010516628585935517, 0.0485030645921735]\n",
      "Episode 4240, Avg rewards: [0.008536430566133535, 0.049245638849599246]\n",
      "Episode 4250, Avg rewards: [0.009086485571634086, 0.04942244224422443]\n",
      "Episode 4260, Avg rewards: [0.013329767042638331, 0.044554455445544566]\n",
      "Episode 4270, Avg rewards: [0.015097800988890096, 0.04454266855256954]\n",
      "Episode 4280, Avg rewards: [0.01674796600539175, 0.04060928070829061]\n",
      "Episode 4290, Avg rewards: [0.017424956781392425, 0.03626777512916127]\n",
      "Episode 4300, Avg rewards: [0.0186036460788936, 0.03478262661430978]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4300.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4300.pt\n",
      "Episode 4310, Avg rewards: [0.02058384409869558, 0.034287577109359285]\n",
      "Episode 4320, Avg rewards: [0.024034189133199033, 0.02947852477555448]\n",
      "Episode 4330, Avg rewards: [0.024034189133199033, 0.036613523989761615]\n",
      "Episode 4340, Avg rewards: [0.02766455216950266, 0.04083323167481584]\n",
      "Episode 4350, Avg rewards: [0.025464332147500462, 0.0408064432816908]\n",
      "Episode 4360, Avg rewards: [0.023861314702898862, 0.04764284120719764]\n",
      "Episode 4370, Avg rewards: [0.023012658408698015, 0.04589838104689589]\n",
      "Episode 4380, Avg rewards: [0.02301265840869801, 0.046535367822496544]\n",
      "Episode 4390, Avg rewards: [0.020124869629820123, 0.05504550455045505]\n",
      "Episode 4400, Avg rewards: [0.018946180332318947, 0.06293486491506294]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4400.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4400.pt\n",
      "Episode 4410, Avg rewards: [0.016965982312516968, 0.058550140728358555]\n",
      "Episode 4420, Avg rewards: [0.013515637278013515, 0.0613004157558613]\n",
      "Episode 4430, Avg rewards: [0.017405311959767404, 0.05704141842755705]\n",
      "Episode 4440, Avg rewards: [0.01683954109696684, 0.05414184275570415]\n",
      "Episode 4450, Avg rewards: [0.016426999842841426, 0.058470847084708476]\n",
      "Episode 4460, Avg rewards: [0.020799937136570797, 0.05921342134213421]\n",
      "Episode 4470, Avg rewards: [0.020578706222270576, 0.05195269526952695]\n",
      "Episode 4480, Avg rewards: [0.022558904242072558, 0.0624980355178375]\n",
      "Episode 4490, Avg rewards: [0.02751463791067751, 0.0596953742993347]\n",
      "Episode 4500, Avg rewards: [0.025864472894175865, 0.052371784797527375]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4500.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4500.pt\n",
      "Episode 4510, Avg rewards: [0.025864472894175865, 0.05763659699303265]\n",
      "Episode 4520, Avg rewards: [0.027278900051177276, 0.06412724605793912]\n",
      "Episode 4530, Avg rewards: [0.030784250586230785, 0.07066968601622067]\n",
      "Episode 4540, Avg rewards: [0.030843185051105844, 0.06692931197881694]\n",
      "Episode 4550, Avg rewards: [0.03208080881348208, 0.06492554017306493]\n",
      "Episode 4560, Avg rewards: [0.02836793752635337, 0.06312214554788813]\n",
      "Episode 4570, Avg rewards: [0.029193020034604195, 0.068001919239543]\n",
      "Episode 4580, Avg rewards: [0.028027326542178026, 0.06752258559189253]\n",
      "Episode 4590, Avg rewards: [0.02408919463374909, 0.056479576529081484]\n",
      "Episode 4600, Avg rewards: [0.02715378680725215, 0.0611943337190862]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4600.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4600.pt\n",
      "Episode 4610, Avg rewards: [0.02715378680725215, 0.06064856485648565]\n",
      "Episode 4620, Avg rewards: [0.026839469661251836, 0.055572342948580565]\n",
      "Episode 4630, Avg rewards: [0.024984284142699983, 0.04950995099509951]\n",
      "Episode 4640, Avg rewards: [0.028115668709728113, 0.048547354735473544]\n",
      "Episode 4650, Avg rewards: [0.027620619204777618, 0.05047254725472547]\n",
      "Episode 4660, Avg rewards: [0.031250982241081256, 0.04278749303501779]\n",
      "Episode 4670, Avg rewards: [0.03171339111933171, 0.04338862457674339]\n",
      "Episode 4680, Avg rewards: [0.03336355613583336, 0.046366779535096374]\n",
      "Episode 4690, Avg rewards: [0.030888308611080887, 0.05107760776077608]\n",
      "Episode 4700, Avg rewards: [0.031124046470581118, 0.04296822539396797]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4700.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4700.pt\n",
      "Episode 4710, Avg rewards: [0.031124046470581125, 0.040999457088566]\n",
      "Episode 4720, Avg rewards: [0.03167410147608167, 0.03931000242881431]\n",
      "Episode 4730, Avg rewards: [0.0313990739733314, 0.04103017444601602]\n",
      "Episode 4740, Avg rewards: [0.027910153652727913, 0.044271570014144265]\n",
      "Episode 4750, Avg rewards: [0.02372973561092373, 0.042523180889517516]\n",
      "Episode 4760, Avg rewards: [0.01778914155151779, 0.04963067735344963]\n",
      "Episode 4770, Avg rewards: [0.016265912305516265, 0.04927707056419928]\n",
      "Episode 4780, Avg rewards: [0.016065892303516065, 0.0456034889203206]\n",
      "Episode 4790, Avg rewards: [0.0173035160658923, 0.03224893917963225]\n",
      "Episode 4800, Avg rewards: [0.015653351049390654, 0.0338991041961339]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4800.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4800.pt\n",
      "Episode 4810, Avg rewards: [0.017303516065892303, 0.039674681753889676]\n",
      "Episode 4820, Avg rewards: [0.0184036260768934, 0.04008722300801509]\n",
      "Episode 4830, Avg rewards: [0.01757854356864258, 0.037686982984012685]\n",
      "Episode 4840, Avg rewards: [0.015048290543340045, 0.03265790864800766]\n",
      "Episode 4850, Avg rewards: [0.021271770034146268, 0.03399375651850899]\n",
      "Episode 4860, Avg rewards: [0.022261869044047266, 0.028586292695203585]\n",
      "Episode 4870, Avg rewards: [0.027306659237352305, 0.03221665573150721]\n",
      "Episode 4880, Avg rewards: [0.02617083136885117, 0.03784565269713785]\n",
      "Episode 4890, Avg rewards: [0.027490963382052487, 0.038945762708138945]\n",
      "Episode 4900, Avg rewards: [0.03134134842055634, 0.046949063038171944]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep4900.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep4900.pt\n",
      "Episode 4910, Avg rewards: [0.02969118340405469, 0.044473815513419473]\n",
      "Episode 4920, Avg rewards: [0.02859107339305359, 0.044941362268094955]\n",
      "Episode 4930, Avg rewards: [0.034225208235109224, 0.04712158028989712]\n",
      "Episode 4940, Avg rewards: [0.037918434700612916, 0.04712158028989712]\n",
      "Episode 4950, Avg rewards: [0.039474304573314474, 0.05031189932180031]\n",
      "Episode 4960, Avg rewards: [0.0374941065535125, 0.053699875482053704]\n",
      "Episode 4970, Avg rewards: [0.03386374351720887, 0.04932693818832432]\n",
      "Episode 4980, Avg rewards: [0.029620462046204624, 0.042244224422442245]\n",
      "Episode 4990, Avg rewards: [0.03011158258683011, 0.044856985698569864]\n",
      "Episode 5000, Avg rewards: [0.028913248467703913, 0.041246981841041246]\n",
      "Saved checkpoint: ./checkpoints\\agent_0_ep5000.pt\n",
      "Saved checkpoint: ./checkpoints\\agent_1_ep5000.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_0_avg_reward</td><td>████▆▅▄▃▄▄▃▂▁▃▃▃▃▃▂▂▂▂▂▁▁▂▂▂▁▂▂▁▁▂▃▃▃▃▃▂</td></tr><tr><td>agent_0_epsilon</td><td>██▇▇▆▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_0_loss</td><td>█▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_0_total_reward</td><td>▁▁▅▁▁█▅▃▁▅▆▆▁▁▁▁▁▁▁▁▁▄▁▁▁▁▄▁▁▁▁▁▆▁▁▁▁▁▁▅</td></tr><tr><td>agent_1_avg_reward</td><td>▇▆▆▅▇▄▄█▆▄▆▅▅▄▃▄▄▃▆▃▂▂▅▅▁▁▂▂▄▄▄▄▅▃▃▃▃▃▄▄</td></tr><tr><td>agent_1_epsilon</td><td>█▆▅▅▄▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_1_loss</td><td>█▃▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>agent_1_total_reward</td><td>▁▃▁▅▅▁▁▅▁█▁▁▁▆▁▁▇▁▁▃▁▁▁▁▁▁▁▁▁▁▁▁▅▃▁▁▁▁▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_0_avg_reward</td><td>0.02891</td></tr><tr><td>agent_0_epsilon</td><td>0.05</td></tr><tr><td>agent_0_loss</td><td>0.00047</td></tr><tr><td>agent_0_total_reward</td><td>0</td></tr><tr><td>agent_1_avg_reward</td><td>0.04125</td></tr><tr><td>agent_1_epsilon</td><td>0.05</td></tr><tr><td>agent_1_loss</td><td>0.00047</td></tr><tr><td>agent_1_total_reward</td><td>0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ineffable-treat-3</strong> at: <a href='https://wandb.ai/marionapla/LBF_proves/runs/ewnv7rqo' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves/runs/ewnv7rqo</a><br> View project at: <a href='https://wandb.ai/marionapla/LBF_proves' target=\"_blank\">https://wandb.ai/marionapla/LBF_proves</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251031_164033-ewnv7rqo\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ===========================\n",
    "# Imports and setup\n",
    "# ===========================\n",
    "import lbforaging\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ===========================\n",
    "# Environment setup\n",
    "# ===========================\n",
    "env_conf = \"Foraging-8x8-2p-4f-v3\"\n",
    "env = gym.make(env_conf)\n",
    "\n",
    "\n",
    "# ===========================\n",
    "# Hyperparameters\n",
    "# ===========================\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 5000\n",
    "MAX_EPISODES = 5000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.999\n",
    "EPSILON_MIN = 0.05\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "BURN_IN = 1000\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "# ===========================\n",
    "# WandB initialization\n",
    "# ===========================\n",
    "run = wandb.init(\n",
    "    project=\"LBF_proves\",\n",
    "    config={\n",
    "        \"env_name\": env_conf,\n",
    "        'learning_rate': LR,\n",
    "        'memory_size': MEMORY_SIZE,\n",
    "        'max_episodes': MAX_EPISODES,\n",
    "        'epsilon_start': EPSILON_START,\n",
    "        'epsilon_decay': EPSILON_DECAY,\n",
    "        'epsilon_min': EPSILON_MIN,\n",
    "        'gamma': GAMMA,\n",
    "        'batch_size': BATCH_SIZE,\n",
    "        'burn_in': BURN_IN,\n",
    "        \n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "'''config={\n",
    "    \"model\": '2-step_DQN',\n",
    "    \"learning_rate\": lr,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"MAX_EPISODES\": MAX_EPISODES,\n",
    "    \"EPSILON_DECAY\": EPSILON_DECAY, \n",
    "    \"batch_size\": BATCH_SIZE\n",
    "})'''\n",
    "\n",
    "# ===========================\n",
    "# Deep Q-Network (DQN)\n",
    "# ===========================\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"A simple 3-layer fully connected Deep Q-Network.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, lr=LR, device=DEVICE):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        if device == 'cuda':\n",
    "            self.model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).to(self.device)\n",
    "        if x.ndim > 2:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.model[-1].out_features)\n",
    "        else:\n",
    "            qvals = self.forward(state)\n",
    "            return torch.argmax(qvals).item()\n",
    "\n",
    "# ===========================\n",
    "# Replay Buffer\n",
    "# ===========================\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Stores past experiences for experience replay.\"\"\"\n",
    "    def __init__(self, capacity=MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition', ['state', 'action', 'reward', 'done', 'next_state'])\n",
    "    \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(self.transition(state, action, reward, done, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        return zip(*batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ===========================\n",
    "# Independent Q-Learning Agent\n",
    "# ===========================\n",
    "class IQLAgent:\n",
    "    \"\"\"Independent Q-Learning agent for multi-agent environments.\"\"\"\n",
    "    def __init__(self, obs_dim, act_dim, device=DEVICE, lr=LR, gamma=GAMMA, epsilon=EPSILON_START, eps_decay=EPSILON_DECAY):\n",
    "        self.qnet = DQN(obs_dim, act_dim, lr, device)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.last_loss = None  # Store last loss for logging\n",
    "    \n",
    "    def store_transition(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "    \n",
    "    def update(self, batch_size=BATCH_SIZE):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.qnet.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(-1).to(self.qnet.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.qnet.device)\n",
    "        dones = torch.BoolTensor(dones).to(self.qnet.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.qnet.device)\n",
    "        \n",
    "        qvals = self.qnet(states).gather(1, actions)\n",
    "        q_next = self.qnet(next_states).max(dim=1)[0].detach()\n",
    "        q_next[dones] = 0\n",
    "        target = rewards + self.gamma * q_next\n",
    "        \n",
    "        loss = nn.MSELoss()(qvals.squeeze(), target)\n",
    "        self.qnet.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.qnet.optimizer.step()\n",
    "\n",
    "        # Store last loss\n",
    "        self.last_loss = loss.item()\n",
    "\n",
    "# ===========================\n",
    "# Multi-Agent IQL Training Loop\n",
    "# ===========================\n",
    "def train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE):\n",
    "    num_agents = env.unwrapped.n_agents\n",
    "    agents = []\n",
    "\n",
    "    for i in range(num_agents):\n",
    "        obs_dim = np.prod(env.observation_space[i].shape)\n",
    "        act_dim = env.action_space[i].n\n",
    "        agents.append(IQLAgent(obs_dim, act_dim, device=device))\n",
    "        print(f\"Initialized Agent {i}: obs_dim={obs_dim}, act_dim={act_dim}\")\n",
    "    \n",
    "    rewards_history = [[] for _ in range(num_agents)]\n",
    "\n",
    "    # ----------- Burn-in Phase -----------\n",
    "    print(\"Burn-in buffer with random actions...\")\n",
    "    steps = 0\n",
    "    while steps < BURN_IN:\n",
    "        print(f\"Burn-in steps: {steps}/{BURN_IN}\", end='\\r')\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [env.action_space[i].sample() for i in range(num_agents)]\n",
    "            next_obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            done = np.any(terminated) or np.any(truncated)\n",
    "\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i].flatten(), actions[i], rewards[i], done, next_obs[i].flatten())\n",
    "            \n",
    "            obs = next_obs\n",
    "            steps += 1\n",
    "    print(\"Burn-in complete.\\nStarting training...\")\n",
    "\n",
    "    total_env_steps = 0\n",
    "    # ----------- Main Training Loop -----------\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        total_rewards = [0] * num_agents\n",
    "\n",
    "        while not done:\n",
    "            actions = []\n",
    "            for i, agent in enumerate(agents):\n",
    "                a = agent.qnet.get_action(obs[i].flatten(), epsilon=agent.epsilon)\n",
    "                actions.append(a)\n",
    "\n",
    "            next_obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "            done = np.any(terminated) or np.any(truncated)\n",
    "            total_env_steps += 1\n",
    "\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i].flatten(), actions[i], rewards[i], done, next_obs[i].flatten())\n",
    "                agent.update()\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Decay epsilon\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_MIN, agent.epsilon * agent.eps_decay)\n",
    "\n",
    "        # -------- Logging --------\n",
    "        avg_rewards = [np.mean(rewards_history[i][-100:] + [total_rewards[i]]) for i in range(num_agents)]\n",
    "        if ep % 10 == 0:\n",
    "            print(f\"Episode {ep}, Avg rewards: {avg_rewards}\")\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            wandb.log({f\"agent_{i}_loss\": agent.last_loss}, step=total_env_steps)\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            wandb.log({\n",
    "                f\"agent_{i}_total_reward\": total_rewards[i],\n",
    "                f\"agent_{i}_avg_reward\": avg_rewards[i],\n",
    "                f\"agent_{i}_epsilon\": agents[i].epsilon,\n",
    "                # f\"agent_{i}_loss\": agents[i].last_loss if agents[i].last_loss is not None else 0.0\n",
    "            }, step=ep)\n",
    "\n",
    "        # -------- Save model checkpoints every 1000 episodes --------\n",
    "        import os\n",
    "        if ep % 100 == 0:\n",
    "            save_dir = \"./checkpoints\"\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            for i, agent in enumerate(agents):\n",
    "                save_path = os.path.join(save_dir, f\"agent_{i}_ep{ep}.pt\")\n",
    "                torch.save(agent.qnet.state_dict(), save_path)\n",
    "                print(f\"Saved checkpoint: {save_path}\")\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            rewards_history[i].append(total_rewards[i])\n",
    "\n",
    "    return agents, rewards_history\n",
    "\n",
    "# ===========================\n",
    "# Run training\n",
    "# ===========================\n",
    "agents, rewards_history = train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE)\n",
    "\n",
    "# ===========================\n",
    "# Optional: Plot learning curves\n",
    "# ===========================\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# for i in range(env.unwrapped.n_agents):\n",
    "#     plt.plot(rewards_history[i], label=f\"Agent {i}\")\n",
    "# plt.xlabel(\"Episode\")\n",
    "# plt.ylabel(\"Total Reward\")\n",
    "# plt.title(\"Learning Curves\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# ===========================\n",
    "# Cleanup\n",
    "# ===========================\n",
    "wandb.finish()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c9f1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "[0, 1]\n",
      "(array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 0., 2., 1., 2., 1.,\n",
      "       1.], dtype=float32), array([1., 1., 1., 1., 4., 2., 4., 3., 1., 6., 2., 1., 2., 1., 1., 0., 2.,\n",
      "       1.], dtype=float32))\n",
      "Total rewards for this episode: [0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Number of agents\n",
    "num_agents = env.unwrapped.n_agents\n",
    "\n",
    "# Observation and action dimensions for each agent\n",
    "agents = []\n",
    "for i in range(num_agents):\n",
    "    obs_dim = np.prod(env.observation_space[i].shape)\n",
    "    act_dim = env.action_space[i].n\n",
    "    agent = IQLAgent(obs_dim, act_dim, device='cpu')  # device can be 'cuda'\n",
    "    \n",
    "    # Load the trained model\n",
    "    checkpoint_path = f\"./checkpoints/agent_{i}_ep5000.pt\"\n",
    "    agent.qnet.load_state_dict(torch.load(checkpoint_path, map_location='cpu'))\n",
    "    \n",
    "    agents.append(agent)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_rewards = [0] * num_agents\n",
    "\n",
    "while not done:\n",
    "    actions = []\n",
    "    for i, agent in enumerate(agents):\n",
    "        # ε = 0 → fully greedy actions\n",
    "        action = agent.qnet.get_action(obs[i].flatten(), epsilon=0.0)\n",
    "        actions.append(action)\n",
    "        \n",
    "    print(actions)\n",
    "    \n",
    "    next_obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "    done = np.any(terminated) or np.any(truncated)\n",
    "\n",
    "    print(next_obs)\n",
    "    # Accumulate rewards\n",
    "    for i in range(num_agents):\n",
    "        total_rewards[i] += rewards[i]\n",
    "\n",
    "    obs = next_obs\n",
    "\n",
    "print(f\"Total rewards for this episode: {total_rewards}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60992cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1]\n",
      "[4, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[2, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[0, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[3, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[5, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[5, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[3, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[1, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[3, 1]\n",
      "[4, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "[5, 1]\n",
      "Average rewards over 10 episodes: [0.02 0.  ]\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "avg_rewards = np.zeros(num_agents)\n",
    "\n",
    "for ep in range(N):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    ep_rewards = np.zeros(num_agents)\n",
    "    \n",
    "    while not done:\n",
    "        actions = [agent.qnet.get_action(obs[i].flatten(), epsilon=0.0) for i, agent in enumerate(agents)]\n",
    "        obs, rewards, terminated, truncated, infos = env.step(actions)\n",
    "        print(actions)\n",
    "        done = np.any(terminated) or np.any(truncated)\n",
    "        ep_rewards += rewards\n",
    "    \n",
    "    avg_rewards += ep_rewards\n",
    "\n",
    "avg_rewards /= N\n",
    "print(f\"Average rewards over {N} episodes: {avg_rewards}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9383fb43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
