{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eadb6122",
   "metadata": {},
   "source": [
    "# LBF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca1919",
   "metadata": {},
   "source": [
    "python3 -m venv lbf-env\n",
    "source lbf-env/bin/activate\n",
    "pip install -e lb-foraging/\n",
    "\n",
    "python -m ipykernel install --user --name=lbf-env --display-name \"Python (lb-foraging)\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba5e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lbforaging in c:\\users\\propietari\\anaconda3\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.26.4)\n",
      "Requirement already satisfied: gymnasium in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.2.1)\n",
      "Requirement already satisfied: pyglet<2 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.5.31)\n",
      "Requirement already satisfied: six in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from lbforaging) (1.16.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (4.15.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\propietari\\anaconda3\\lib\\site-packages (from gymnasium->lbforaging) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lbforaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22cae2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lbforaging \n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "845306bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Discrete(6), Discrete(6))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0329921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tuple(Box([-1. -1.  0. -1. -1.  0. -1. -1.  0.], [7. 7. 4. 7. 7. 2. 7. 7. 2.], (9,), float32), Box([-1. -1.  0. -1. -1.  0. -1. -1.  0.], [7. 7. 4. 7. 7. 2. 7. 7. 2.], (9,), float32))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3601d010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n"
     ]
    }
   ],
   "source": [
    "num_agents = env.unwrapped.n_agents\n",
    "print(f'Number of agents: {num_agents}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2ba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "wandb: Currently logged in as: marionapla to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\propietari\\Desktop\\TGF-MARL\\PROVES\\wandb\\run-20251029_180321-ke5v7o3s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/marionapla/LBF/runs/ke5v7o3s' target=\"_blank\">super-sun-32</a></strong> to <a href='https://wandb.ai/marionapla/LBF' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/marionapla/LBF' target=\"_blank\">https://wandb.ai/marionapla/LBF</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/marionapla/LBF/runs/ke5v7o3s' target=\"_blank\">https://wandb.ai/marionapla/LBF/runs/ke5v7o3s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<OrderEnforcing<PassiveEnvChecker<ForagingEnv<Foraging-8x8-2p-4f-v3>>>>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ---------------- Hyperparameters ----------------\\nLR = 1e-3\\nMEMORY_SIZE = 50000\\nMAX_EPISODES = 20000\\nEPSILON_START = 1.0\\nEPSILON_DECAY = 0.99\\nEPSILON_MIN = 0.05\\nGAMMA = 0.99\\nBATCH_SIZE = 64\\nBURN_IN = 500\\nDEVICE = \\'cpu\\'  # \\'cuda\\' if GPU available\\n\\n# ---------------- DQN Network ----------------\\nclass DQN(nn.Module):\\n    def __init__(self, obs_dim, act_dim, lr=LR, device=DEVICE):\\n        super(DQN, self).__init__()\\n        self.device = device\\n        self.model = nn.Sequential(\\n            nn.Linear(obs_dim, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, act_dim)\\n        )\\n        self.optimizer = optim.Adam(self.parameters(), lr=lr)\\n        if device == \\'cuda\\':\\n            self.model.cuda()\\n        \\n    def forward(self, x):\\n        x = torch.FloatTensor(x).to(self.device)\\n        return self.model(x)\\n    \\n    def get_action(self, state, epsilon=0.05):\\n        if np.random.random() < epsilon:\\n            return np.random.randint(self.model[-1].out_features)\\n        else:\\n            qvals = self.forward(state)\\n            return torch.argmax(qvals).item()\\n\\n# ---------------- Replay Buffer ----------------\\nclass ReplayBuffer:\\n    def __init__(self, capacity=MEMORY_SIZE):\\n        self.buffer = deque(maxlen=capacity)\\n        self.transition = namedtuple(\\'Transition\\', [\\'state\\', \\'action\\', \\'reward\\', \\'done\\', \\'next_state\\'])\\n    \\n    def append(self, state, action, reward, done, next_state):\\n        self.buffer.append(self.transition(state, action, reward, done, next_state))\\n    \\n    def sample(self, batch_size):\\n        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\\n        batch = [self.buffer[i] for i in idxs]\\n        return zip(*batch)\\n    \\n    def __len__(self):\\n        return len(self.buffer)\\n\\n# ---------------- Independent Q-Learning Agent ----------------\\nclass IQLAgent:\\n    def __init__(self, obs_dim, act_dim, device=DEVICE, lr=LR, gamma=GAMMA, epsilon=EPSILON_START, eps_decay=EPSILON_DECAY):\\n        self.qnet = DQN(obs_dim, act_dim, lr, device)\\n        self.buffer = ReplayBuffer()\\n        self.gamma = gamma\\n        self.epsilon = epsilon\\n        self.eps_decay = eps_decay\\n    \\n    def store_transition(self, state, action, reward, done, next_state):\\n        self.buffer.append(state, action, reward, done, next_state)\\n    \\n    def update(self, batch_size=BATCH_SIZE):\\n        if len(self.buffer) < batch_size:\\n            return\\n        states, actions, rewards, dones, next_states = self.buffer.sample(batch_size)\\n        states = torch.FloatTensor(states).to(self.qnet.device)\\n        actions = torch.LongTensor(actions).unsqueeze(-1).to(self.qnet.device)\\n        rewards = torch.FloatTensor(rewards).to(self.qnet.device)\\n        dones = torch.BoolTensor(dones).to(self.qnet.device)\\n        next_states = torch.FloatTensor(next_states).to(self.qnet.device)\\n        \\n        qvals = self.qnet(states).gather(1, actions)\\n        q_next = self.qnet(next_states).max(dim=1)[0].detach()\\n        q_next[dones] = 0\\n        target = rewards + self.gamma * q_next\\n        \\n        loss = nn.MSELoss()(qvals.squeeze(), target)\\n        self.qnet.optimizer.zero_grad()\\n        loss.backward()\\n        self.qnet.optimizer.step()\\n\\n# ---------------- Multi-Agent Training Loop ----------------\\ndef train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE):\\n    num_agents = env.unwrapped.n_agents\\n    agents = []\\n    for i in range(num_agents):\\n        obs_dim = env.observation_space[i].shape[0]\\n        act_dim = env.action_space[i].n\\n        agents.append(IQLAgent(obs_dim, act_dim, device=device))\\n    \\n    rewards_history = [[] for _ in range(num_agents)]\\n\\n    # Optional burn-in\\n    print(\"Burn-in buffer with random actions...\")\\n    steps = 0\\n    \\n    while steps < BURN_IN:\\n        print(\\'new episode bc done\\')\\n        obs = env.reset()\\n        done = False\\n        while not done:\\n            actions = [env.action_space[i].sample() if not done else 0 for i in range(num_agents)]\\n            print(actions, steps)\\n            next_obs, rewards, done, info, __ = env.step(actions)\\n            for i, agent in enumerate(agents):\\n                agent.store_transition(obs[i], actions[i], rewards[i], done, next_obs[i])\\n            obs = next_obs\\n            steps += 1\\n    print(\"Burn-in complete.\\nStarting training...\")\\n\\n    # Main training loop\\n    for ep in range(1, n_episodes + 1):\\n        obs = env.reset()\\n        done = [False] * num_agents\\n        total_rewards = [0] * num_agents\\n\\n        while not done:\\n            # Select actions\\n            actions = []\\n            for i, agent in enumerate(agents):\\n                if not done:\\n                    a = agent.qnet.get_action(obs[i], epsilon=agent.epsilon)\\n                else:\\n                    a = 0\\n                actions.append(a)\\n\\n            # Step environment\\n            next_obs, rewards, done, infos, __ = env.step(actions)\\n\\n            # Store transitions & train agents\\n            for i, agent in enumerate(agents):\\n                agent.store_transition(obs[i], actions[i], rewards[i], done, next_obs[i])\\n                agent.update()\\n                total_rewards[i] += rewards[i]\\n\\n            obs = next_obs\\n\\n        # Decay epsilon\\n        for agent in agents:\\n            agent.epsilon = max(EPSILON_MIN, agent.epsilon * agent.eps_decay)\\n\\n        # Logging\\n        if ep % 10 == 0:\\n            avg_rewards = [np.mean(rewards_history[i][-100:] + [total_rewards[i]]) for i in range(num_agents)]\\n            print(f\"Episode {ep}, Avg rewards: {avg_rewards}\")\\n            for i, avg in enumerate(avg_rewards):\\n                wandb.log({f\"agent_{i}_avg_reward\": avg}, step=ep)\\n\\n        for i in range(num_agents):\\n            rewards_history[i].append(total_rewards[i])\\n\\n    return agents, rewards_history\\n\\n# ---------------- Train ----------------\\nagents, rewards_history = train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE)\\n\\n# ---------------- Plot learning curves ----------------\\nplt.figure(figsize=(10, 5))\\nfor i in range(env.unwrapped.n_agents):\\n    plt.plot(rewards_history[i], label=f\"Agent {i}\")\\nplt.xlabel(\"Episode\")\\nplt.ylabel(\"Total Reward\")\\nplt.title(\"Learning Curves\")\\nplt.legend()\\nplt.show()\\n\\n# ---------------- Cleanup ----------------\\nwandb.finish()\\nenv.close()\\n\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lbforaging \n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "\n",
    "# ---------------- WandB setup ----------------\n",
    "run = wandb.init(\n",
    "    project=\"LBF\",\n",
    "    config={\n",
    "        \"env_name\": \"Foraging-8x8-2p-4f-v3\",\n",
    "    },\n",
    "    sync_tensorboard=True,\n",
    "    save_code=True,\n",
    ")\n",
    "\n",
    "# ---------------- Environment ----------------\n",
    "env_conf = \"Foraging-8x8-2p-4f-v3\"\n",
    "env = gym.make(env_conf)\n",
    "'''env = gym.make(\"Foraging<obs>-<x_size>x<y_size>-<n_agents>p-<food>f<force_c>-v1\")\n",
    "    • <obs>: This optional field can either be empty (\"\") or indicate a partially observable task\n",
    "    with visibility radius of two fields (\"-2s).\n",
    "    • <x_size>: This field indicates the horizontal size of the environment map and can by\n",
    "    default take any values between 5 and 20.\n",
    "    • <y_size>: This field indicates the vertical size of the environment map and can by default\n",
    "    take any values between 5 and 20. It should be noted, that upon import only environments\n",
    "    with square dimensions (<x_size> = <y_size>) are registered and ready for creation.\n",
    "    • <n_agents>: This field indicates the number of agents within the environment. By default,\n",
    "    any values between 2 and 5 are automatically registered.\n",
    "    • <food>: This field indicates the number of food items scattered within the environment. It\n",
    "    can take any values between 1 and 10 by default.\n",
    "    • <force_c>: This optional field can either be empty (\"\") or indicate a task with only\n",
    "    \"cooperative food\" (\"-coop\". In the latter case, the environment will only contain food of a\n",
    "    level such that all agents have to cooperate in order to pick the food up. This mode should\n",
    "    only be used with up to four agents.'''\n",
    "\n",
    "# ---------------- Hyperparameters ----------------\n",
    "LR = 1e-3\n",
    "MEMORY_SIZE = 50000\n",
    "MAX_EPISODES = 20000\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.99\n",
    "EPSILON_MIN = 0.05\n",
    "GAMMA = 0.99\n",
    "BATCH_SIZE = 64\n",
    "BURN_IN = 500\n",
    "DEVICE = 'cpu'  # 'cuda' if GPU available\n",
    "\n",
    "# ---------------- DQN Network ----------------\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, lr=LR, device=DEVICE):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, act_dim)\n",
    "        )\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\n",
    "        if device == 'cuda':\n",
    "            self.model.cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.FloatTensor(x).to(self.device)\n",
    "        return self.model(x)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.model[-1].out_features)\n",
    "        else:\n",
    "            qvals = self.forward(state)\n",
    "            return torch.argmax(qvals).item()\n",
    "\n",
    "# ---------------- Replay Buffer ----------------\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=MEMORY_SIZE):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.transition = namedtuple('Transition', ['state', 'action', 'reward', 'done', 'next_state'])\n",
    "    \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(self.transition(state, action, reward, done, next_state))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        idxs = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in idxs]\n",
    "        return zip(*batch)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ---------------- Independent Q-Learning Agent ----------------\n",
    "class IQLAgent:\n",
    "    def __init__(self, obs_dim, act_dim, device=DEVICE, lr=LR, gamma=GAMMA, epsilon=EPSILON_START, eps_decay=EPSILON_DECAY):\n",
    "        self.qnet = DQN(obs_dim, act_dim, lr, device)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "    \n",
    "    def store_transition(self, state, action, reward, done, next_state):\n",
    "        self.buffer.append(state, action, reward, done, next_state)\n",
    "    \n",
    "    def update(self, batch_size=BATCH_SIZE):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        states, actions, rewards, dones, next_states = self.buffer.sample(batch_size)\n",
    "        states = torch.FloatTensor(states).to(self.qnet.device)\n",
    "        actions = torch.LongTensor(actions).unsqueeze(-1).to(self.qnet.device)\n",
    "        rewards = torch.FloatTensor(rewards).to(self.qnet.device)\n",
    "        dones = torch.BoolTensor(dones).to(self.qnet.device)\n",
    "        next_states = torch.FloatTensor(next_states).to(self.qnet.device)\n",
    "        \n",
    "        qvals = self.qnet(states).gather(1, actions)\n",
    "        q_next = self.qnet(next_states).max(dim=1)[0].detach()\n",
    "        q_next[dones] = 0\n",
    "        target = rewards + self.gamma * q_next\n",
    "        \n",
    "        loss = nn.MSELoss()(qvals.squeeze(), target)\n",
    "        self.qnet.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.qnet.optimizer.step()\n",
    "\n",
    "# ---------------- Multi-Agent Training Loop ----------------\n",
    "def train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE):\n",
    "    num_agents = env.unwrapped.n_agents\n",
    "    agents = []\n",
    "    for i in range(num_agents):\n",
    "        obs_dim = env.observation_space[i].shape[0]\n",
    "        act_dim = env.action_space[i].n\n",
    "        agents.append(IQLAgent(obs_dim, act_dim, device=device))\n",
    "    \n",
    "    rewards_history = [[] for _ in range(num_agents)]\n",
    "\n",
    "    # Optional burn-in\n",
    "    print(\"Burn-in buffer with random actions...\")\n",
    "    steps = 0\n",
    "    \n",
    "    while steps < BURN_IN:\n",
    "        print('new episode bc done')\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = [env.action_space[i].sample() if not done else 0 for i in range(num_agents)]\n",
    "            print(actions, steps)\n",
    "            next_obs, rewards, done, info, __ = env.step(actions)\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i], actions[i], rewards[i], done, next_obs[i])\n",
    "            obs = next_obs\n",
    "            steps += 1\n",
    "    print(\"Burn-in complete.\\nStarting training...\")\n",
    "\n",
    "    # Main training loop\n",
    "    for ep in range(1, n_episodes + 1):\n",
    "        obs = env.reset()\n",
    "        done = [False] * num_agents\n",
    "        total_rewards = [0] * num_agents\n",
    "\n",
    "        while not done:\n",
    "            # Select actions\n",
    "            actions = []\n",
    "            for i, agent in enumerate(agents):\n",
    "                if not done:\n",
    "                    a = agent.qnet.get_action(obs[i], epsilon=agent.epsilon)\n",
    "                else:\n",
    "                    a = 0\n",
    "                actions.append(a)\n",
    "\n",
    "            # Step environment\n",
    "            next_obs, rewards, done, infos, __ = env.step(actions)\n",
    "\n",
    "            # Store transitions & train agents\n",
    "            for i, agent in enumerate(agents):\n",
    "                agent.store_transition(obs[i], actions[i], rewards[i], done, next_obs[i])\n",
    "                agent.update()\n",
    "                total_rewards[i] += rewards[i]\n",
    "\n",
    "            obs = next_obs\n",
    "\n",
    "        # Decay epsilon\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_MIN, agent.epsilon * agent.eps_decay)\n",
    "\n",
    "        # Logging\n",
    "        if ep % 10 == 0:\n",
    "            avg_rewards = [np.mean(rewards_history[i][-100:] + [total_rewards[i]]) for i in range(num_agents)]\n",
    "            print(f\"Episode {ep}, Avg rewards: {avg_rewards}\")\n",
    "            for i, avg in enumerate(avg_rewards):\n",
    "                wandb.log({f\"agent_{i}_avg_reward\": avg}, step=ep)\n",
    "\n",
    "        for i in range(num_agents):\n",
    "            rewards_history[i].append(total_rewards[i])\n",
    "\n",
    "    return agents, rewards_history\n",
    "\n",
    "# ---------------- Train ----------------\n",
    "agents, rewards_history = train_iql(env, n_episodes=MAX_EPISODES, device=DEVICE)\n",
    "\n",
    "# ---------------- Plot learning curves ----------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(env.unwrapped.n_agents):\n",
    "    plt.plot(rewards_history[i], label=f\"Agent {i}\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"Learning Curves\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ---------------- Cleanup ----------------\n",
    "wandb.finish()\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e349f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
